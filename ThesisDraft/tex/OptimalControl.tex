\chapter{Quantum Optimal Control Theory}
The fundamental problem of Quantum Optimal Control Theory is to steer the dynamics of a quantum system in a desired way through external fields \cite{Rice2000,Shapiro2003}. Often, the goal is the transfer from an initial state, $\ket{\psi_0}$, to the desired target-state, $\ket{\psi_{\mathrm{target}}}$. The fields responsible for controlling the dynamics of the system are parametrised by a set of control parameters or functions. Optimal control theory determines the parameters, which leads to the desired dynamics of the system \cite{Werschnik2007}.\\ 
In control problems, the Hamiltonian of the system is given as
\begin{equation}
	\hat{H} = \hat{H}_0 + \hat{H}_I = \hat{H}_0 + \sum_{n = 1}^{m} \hat{V}_i (u_n(t)) \; ,
	\label{eq:ControlHamiltonians}
\end{equation} 
where $\hat{H}_0$ is an uncontrollable drift of the Hamiltonian, $\hat{H}_i$ are the controllable fields, and $u_n(t)$ are the control functions or parameters. A quantum system like this is completely controllable if every unitary operator $\hat{U}$ is accessible from the identity operator $\hat{I}$ via a path $\gamma (t) = \hat{U}(t, t_0)$ satisfying \cite{Schirmer2001}
\begin{equation}
	i \partial_t \hat{U}(t, t_0) = \left( \hat{H}_0 + \hat{H}_I \right) \hat{U}(t, t_0) \; .
\end{equation} 
For an $N$-dimensional Hilbert spaces, a sufficient condition for complete controllability of a quantum system is that the Lie algebra generated by the Hamiltonians in eq. \eqref{eq:ControlHamiltonians},
\begin{equation}
	L_0 = \mathrm{Lie} \left( i \hat{H}_0, i \hat{H}_1 , \ldots , i \hat{H}_m \right) \; ,
\end{equation}
is of dimension $N^2$ \cite{Ramakrishna1995}.\\
Extending these conditions to infinite-dimensional Hilbert spaces and constrained controls is non-trivial \cite{Huang1983}. Therefore, no calculations of the Lie dimensions will be performed for the control problems presented in this thesis.


\section{The Gradient-Ascent Pulse Engineering Method}
In this instance, the control problem is steering an initial state to a target-state. More specifically, the transfer of a Superfluid to a Mott-Insulator, which is controlled by varying the lattice depth. Therefore, the optimal control problem can be stated as follows: 
Suppose the system is initially described by the state $\ket{\psi_0} = \ket{\psi (0)}$, and the potential is varied in the time interval $[ 0 , T]$. The goal is finding the set of control parameters, $\boldsymbol{u}(t)$, which brings the initial state as close as possible to the target state, $\ket{\psi_{\mathrm{target}}}$. This is formulated in terms of a cost function
\begin{equation}
	J_T = \frac{1}{2} \left( 1-|\braket{\psi_{\mathrm{target}} | \psi (T)}|^2 \right) \; ,
	\label{eq:infidelityCost}
\end{equation}
which is given as half the infidelity between the target and the state at $t=T$. The cost function becomes zero, when the terminal state matches the target state up to an arbitrary phase. Hence, the optimal control problem can be formulated as a minimization problem of eq. \eqref{eq:infidelityCost} \cite{Jager2014}.\\
Experimentally, large variations in the control parameter is often hard to achieve. Therefore, an extra term is often added to the cost function, which penalizes strong variations in the control. The new cost function reads
\begin{equation}
	J = J_T + \frac{\gamma}{2} \sum_{n=1}^{m} \int_{0}^{T} \left( \pdv{u_n}{t} \right)^2 \mathrm{d}t \; ,
	\label{eq:grapeCost}
\end{equation}
where $\gamma$ weighs the relative importance between matching states and smoothness of the control \cite{Jager2014}. As the state transfer is considered the highest priority, $\gamma$ is set as $\gamma \ll 1$, such that $J_T$ dominates the cost function of eq. \eqref{eq:grapeCost}.\\

A powerful way of performing optimal control is through the Gradient-Ascent Pulse Engineering (GRAPE) method. Through GRAPE, the gradient of the cost function \eqref{eq:grapeCost} can be evaluated and used to update the existing set of controls \cite{Khaneja2005}. Thereby, one achieves an optimization of the cost function.\\
The gradient of the cost function can be derived in multiple ways. A common method \cite{Hohenester2007, Winckel2008, BECcontrol} is introducing a Lagrange multiplier, which forces the dynamics to obey the SchrÃ¶dinger equations. The derivation utilizes functional derivatives, as time, $\ket{\psi (t)}$, and the Lagrange multiplier are considered continuous functions. Later, the functions are discretized to perform numerical computations. Although this procedure is fairly easy, higher order correction terms of the gradient are absent due to the continuity of the functions.\\
Here, an alternative derivation following \cite{Khaneja2005, deFouquieres2011} is presented.

Assume the transfer time, $T$, is discretized in $M$ equal steps, $\Delta t = T/M$. Therefore, the control become similarly discretized as
\begin{align}
	u_n = \left( u_n (t_1) , \ldots , u_n (t_M)  \right) \equiv \left( u_1 , \ldots , u_M  \right) \; ,
\end{align}
where $m = 1$ has been assumed in the last statement for cleaner notation. Thus, the system is controlled by only a single parameter. Extending this formalism to $m$ controls is trivial.
Likewise, the time-evolution of the system during the time step $j$ is given by the propagator
\begin{equation}
	\hat{\mathcal{U}}_j \equiv \hat{\mathcal{U}} (u(t_j)) = \exp \lbrace -i \left(  \hat{H}_0 + \hat{V} (u(t_j))  \right) \Delta t \rbrace \; . 
\end{equation} 
Thereby, the cost function \eqref{eq:grapeCost} becomes
\begin{equation}
	J = \frac{1}{2} \left( 1 - |\braket{\psi_{\mathrm{target}} | \prod_{j}^{M} \hat{\mathcal{U}}_j | \psi (0)}|^2 \right) + \frac{\gamma}{2} \sum_{j}^{M-1} \left( \frac{\Delta u_j}{\Delta t} \right)^2 \Delta t \; ,
	\label{eq:discreteCost}
\end{equation}
where $\Delta u_j =  u_{j+1} - u_j$.\\
By defining $c \equiv \braket{\psi_{\mathrm{target}} | \psi (T)}$, the derivative with respect to the control of the first term of the discretized cost \eqref{eq:discreteCost} can be written as 
\begin{equation}
	\frac{\partial}{\partial u_j} J_T = \frac{\partial}{\partial u_j} \frac{1}{2} |\braket{\psi_{\mathrm{target}} | \prod_{j}^{M} \hat{\mathcal{U}}_j | \psi (0)}|^2  = \Re \left( c^* \frac{\partial c}{\partial u_j} \right) \; .
\end{equation}
Focusing on $\frac{\partial c}{\partial u_j}$ reveals
\begin{align}
	\frac{\partial c}{\partial u_j} &= \frac{\partial }{\partial u_j} braket{\psi_{\mathrm{target}} | \prod_{j}^{M} \hat{\mathcal{U}}_j | \psi (0)} \nonumber \\
	&= \bra{\psi_{\mathrm{target}}} \hat{\mathcal{U}}_M \ldots \hat{\mathcal{U}}_{j+1} \frac{\partial \hat{\mathcal{U}}_{j}}{\partial u_j} \hat{\mathcal{U}}_{j-1} \ldots \hat{\mathcal{U}}_{1} 
\end{align}


\section{Interior Point Method}
Through the GRAPE algorithm, the gradient of the cost is calculated. Thus, gradient-based optimization algorithm can be employed in the search for the control, which minimizes the cost functional. 


\section{Quantum Speed Limit}
A subtlety of the above problem is that one is only searching for the control, $\boldsymbol{u}(t)$, which steers the initial state into the target-state at time $t = T$. It is often desirable to obtain the desired state in the shortest timespan possible, however, if a solution exists at $t= T_1 > T_2$, it might not exist at $t = T_2$. The shortest duration for which a solution can be found is called the \textit{quantum speed limit} (QSL). This is due to the fact that quantum mechanics dictates that there is a limit of how many orthogonal states a system can pass through per unit time. A large energy difference to orthogonal states allows for fast oscillations within the system, however, as these differences cannot be arbitrarily large, a lower bound of how fast a system can evolve exists, which in turn leads to the QSL \cite{Caneva2009}.\\
There are several ways of approximating the quantum speed limit, however, there is no known way to reliably estimate the QSL for a general state. Thus, the best option is often to just solve the problem at increasingly shorter durations until a solution no longer can be found [JJ]. 
If the initial state,$\ket{\psi_0}$, and the target-state, $\ket{\psi_{\mathrm{target}}}$, are orthogonal, one can estimate the QSL from the orthogonalization time, which is how long it takes for a state to become orthogonal to itself.
Consider $\ket{\psi (0)} = \sum_{n}^{\infty} c_n \ket{\phi _n}$, where $\hat{H} \ket{\phi _n} = E_n \ket{\phi _n}$. Following \cite{QSLtoffoli} the norm squared of the survival probability is given as
\begin{align}
	|S(t)|^2 = |\braket{\psi (0) | \psi (t)}|^2 &= \sum_{n , m = 0}^{\infty} |c_n|^2 |c_m|^2 \cos \left( (E_n - E_m) t \right) \nonumber \\
	&\geq 1 + \frac{4 t}{\pi ^2} \dv{|S(t)|^2}{t} - \frac{4 t^2}{\pi} \Delta E^2 \; ,
\end{align}
where the trigonometric inequality $\cos x \geq 1 - \left( 4 x \sin x - 2 x^2 \right) / \pi^2$ was used, and $\Delta E$ is the energy spread of the state.
Since $|S(t)|^2 \geq 0$, then $\dv{|S(t)|^2}{t} = 0$ whenever $|S(t)| = 0$, which is the case at the orthogonalization time $t = \tau$. This leaves the inequality $0 \geq 1 - 4 \tau^2 \Delta E^2 / \pi^2$, which yields the Mandelstram Tamm bound when solved \cite{Mandelstam1991}
\begin{equation}
	\tau_{\mathrm{MT}} \geq \frac{\pi}{2 \Delta E} \; .
\end{equation}
This sets a lower bound of the orthogonalization time, however, the bound was derived using a constant Hamiltonian. In the case of optimal control the Hamiltonian is time dependent, which can be taken into account by using arguments from differential geometry \cite{Aharonov,beyondQSL}
\begin{equation}
	\tau_{\mathrm{MT}} \geq \frac{\pi}{2} \left( \int_{0}^{T} \Delta E \; \mathrm{d}t \right) ^{-1} \; , \label{eq:MTlimit}
\end{equation}
From this expression it is clear, that fast solutions require a large value of $\Delta E$, as described earlier. Since \ref{eq:MTlimit} is dependent on the control $\boldsymbol{u}(t)$, one would have to take an infimum over all controls connecting the initial and target state in order to evaluate the lowest value of the bound. This in itself is a task just as difficult as solving the control problem.


\section{Case Study of Frank article ??}
%% discuss figure of merit 
%% from Maja & Julie: infidelity much stronger figure of merit


