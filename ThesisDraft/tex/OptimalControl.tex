\chapter{Quantum Optimal Control Theory}
The fundamental problem of Quantum Optimal Control Theory is to steer the dynamics of a quantum system in a desired way through external fields \cite{Rice2000,Shapiro2003}. Often, the goal is the transfer from an initial state, $\ket{\psi_0}$, to the desired target-state, $\ket{\psi_{\mathrm{target}}}$. The fields responsible for controlling the dynamics of the system are parametrised by a set of control parameters or functions. Optimal control theory determines the parameters, which leads to the desired dynamics of the system \cite{Werschnik2007}.\\ 
In control problems, the Hamiltonian of the system is given as
\begin{equation}
	\hat{H} = \hat{H}_0 + \hat{H}_I = \hat{H}_0 + \sum_{n = 1}^{m} u_n(t) \hat{H}_i \; ,
	\label{eq:ControlHamiltonians}
\end{equation} 
where $\hat{H}_0$ is an uncontrollable drift of the Hamiltonian, $\hat{H}_i$ are the controllable fields, and $u_n(t)$ are the control functions or parameters. A quantum system like this is completely controllable if every unitary operator $\hat{U}$ is accessible from the identity operator $\hat{I}$ via a path $\gamma (t) = \hat{U}(t, t_0)$ satisfying \cite{Schirmer2001}
\begin{equation}
	i \partial_t \hat{U}(t, t_0) = \left( \hat{H}_0 + \hat{H}_I \right) \hat{U}(t, t_0) \; .
\end{equation} 
For an $N$-dimensional Hilbert spaces, a sufficient condition for complete controllability of a quantum system is that the Lie algebra generated by the Hamiltonians in eq. \eqref{eq:ControlHamiltonians},
\begin{equation}
	L_0 = \mathrm{Lie} \left( i \hat{H}_0, i \hat{H}_1 , \ldots , i \hat{H}_m \right) \; ,
\end{equation}
is of dimension $N^2$ \cite{Ramakrishna1995}.\\
Extending these conditions to infinite-dimensional Hilbert spaces and constrained controls is non-trivial \cite{Huang1983}. Therefore, no calculations of the Lie dimensions will be performed for the control problems presented in this thesis.


\section{The Gradient-Ascent Pulse Engineering Method}
In this instance, the control problem is steering an initial state to a target-state. More specifically, the transfer of a Superfluid to a Mott-Insulator, which is controlled by varying the lattice depth. Therefore, the optimal control problem can be stated as follows: 
Suppose the system is initially described by the state $\ket{\psi_0}$, and the potential is varied in the time interval $[ 0 , T]$. The goal is finding the set of control parameters, $\boldsymbol{u}(t)$, which brings the initial state as close as possible to the target state, $\ket{\psi_{\mathrm{target}}}$. This is formulated in terms of a cost function
\begin{equation}
	J_T = \frac{1}{2} \left( 1-|\braket{\psi_{\mathrm{target}} | \psi (T)}|^2 \right) \; ,
	\label{eq:infidelityCost}
\end{equation}
which is given as half the infidelity between the target and the state at $t=T$. The cost function becomes zero, when the terminal state matches the target state up to an arbitrary phase. Hence, the optimal control problem can be formulated as a minimization problem of eq. \eqref{eq:infidelityCost} \cite{Jager2014}.\\
Experimentally, large variations in the control parameter is often hard to achieve. Therefore, an extra term is often added to the cost function, which penalizes strong variations in the control. The new cost function reads
\begin{equation}
	J = J_T + \frac{\gamma}{2} \sum_{n=1}^{m} \int_{0}^{T} \left( \pdv{u_n}{t} \right)^2 \mathrm{d}t \; ,
	\label{eq:grapeCost}
\end{equation}
where $\gamma$ weighs the relative importance between matching states and smoothness of the control \cite{Jager2014}. As the state transfer is considered the highest priority, $\gamma$ is set as $\gamma \ll 1$, such that $J_T$ dominates the cost function of eq. \eqref{eq:grapeCost}.\\

A powerful way of performing optimal control is through the Gradient-Ascent Pulse Engineering (GRAPE) method. Through GRAPE, the gradient of the cost function \eqref{eq:grapeCost} can be evaluated and used to update the existing set of controls \cite{Khaneja2005}. Thereby, one achieves an optimization of the cost function.\\
During the transfer from the initial to the target state, the system must fulfill the Schrödinger equation. This is a constraint of the optimization problem, which is managed by the introduction of a Lagrange multiplier, $\ket{\chi}$. The optimization Lagrangian reads
\begin{equation}
	L = J + \Re \left[ \int_{0}^{T} \bra{\chi} \left( i \ket{\dot{\psi}} - \hat{H} \ket{\psi} \right) \mathrm{d}t \right] \; ,
	\label{eq:optLagrange}
\end{equation} 
where $\ket{\dot{\psi}}$ is the time-derivative of the state $\ket{\psi (t)}$. Note, the explicit time dependence is omitted in favour of cleaner notation. The optimal solutions are the stationary points of $L$ \cite{Hohenester2007}, where
\begin{equation}
	\frac{\delta L}{\delta \chi ^* (t')} = \frac{\delta L}{\delta \psi ^* (t')} = \frac{\delta L}{\delta u_n (t')} = 0 \quad \mathrm{for} \quad  n = 1, \ldots , m \; , \label{eq:statpoint}
\end{equation}
and $0 \leq t' \leq T$. As $\ket{\chi}$, $\ket{\psi}$ and $u_n$ are all considered functions, one must employ the functional derivative.
Calculating the derivative of eq. \eqref{eq:optLagrange} with respect to the Lagrange multiplier, $\bra{\chi (t')}$, yields
\begin{equation}
	\frac{\delta L}{\delta \chi ^* (t')} = \frac{1}{2} \left( i \ket{\dot{\psi}} -  \hat{H} \ket{\psi} \right) \; ,
\end{equation}
or when rewritten
\begin{equation}
	 i \ket{\dot{\psi}} =  \hat{H} \ket{\psi} \; ,
\end{equation}
which is the Schrödinger equation for the state $\ket{\psi (t)}$.
Taking the derivative of eq. \eqref{eq:optLagrange} with respect to $\bra{\psi (t')}$ yields
\begin{align}
	\frac{\delta L}{\delta \psi ^* (t')} &= \frac{\delta }{\delta \psi ^* (t')} \left[ \frac{1}{2} \int_{0}^{T} \left( i \braket{\dot{\psi} | \chi} -  \bra{\psi} \hat{H} \ket{\psi} \right) \mathrm{d}t \right] \nonumber \\ 
	&= - \frac{1}{2} H \ket{\chi} + \frac{\delta }{\delta \psi ^* (t')} \left[ \frac{i}{2} \left( \braket{\psi | \chi} \Big|_0^T - \int_0^T \braket{\psi | \dot{\chi}} \right) \right] \nonumber \\
	&= - \frac{1}{2} \left( H \ket{\chi} -i \ket{\dot{\chi}}  \right) \; . \label{eq:deriv_psit}
\end{align}
Furthermore, one must consider the derivative with respect to $\bra{\psi (T)}$ due to explicit dependence in the fidelity, whereby
\begin{equation}
	\frac{\delta L}{\delta \psi ^* (T)} = - \frac{1}{2} \left( \ket{\psi_{\mathrm{target}}} \braket{\psi_{\mathrm{target}} | \psi (T)} - i \ket{\chi (T)} \right) \; . \label{eq:deriv_psiT}
\end{equation}
From the requirements of the derivatives of eq. \eqref{eq:statpoint}, equation \eqref{eq:deriv_psit} can be reshaped to Schrödinger equation for $\ket{\chi (t')}$,
\begin{equation}
	 i \ket{\dot{\chi}} =  - \hat{H} \ket{\chi} \; .
	 \label{eq:LagrangeSchroedinger}
\end{equation}
However, the sign of the Hamiltonian is flipped implying a backwards propagation in time. Further explanation is found when examining eq. \eqref{eq:LagrangeSchroedinger}, which states
\begin{equation}
	 \ket{\chi (T)} = -i \ket{\psi_{\mathrm{target}}} \braket{\psi_{\mathrm{target}} | \psi (T)} \; .
\end{equation}
Thus, at $t = T$, $\ket{\chi}$ is given as the projection of the final state unto the target-state. Therefore, the Lagrange multiplier, $\ket{\chi}$, can be interpreted as the backwards time-evolved target state \cite{BECcontrol}.\\


%% --------------- corrected until here -------------
%% derive derivative with respect to u_n and include commutator corrections 
%% add algorithmic description as in GRAPE paper
%% mention discretization of time
%% add schematic of control amplitude and its optimization (perhaps from Jesper note)

Finally, the derivative of the optimization Lagrangian with respect to the control parameters is 
\begin{align}
	\frac{\delta L}{\delta u_n (t')} &= - \Re \bra{\chi} \pdv{\hat{H}}{u_n (t')} \ket{\psi} + \frac{\gamma}{2} \frac{\delta }{\delta u_n (t')} \left[ u_n \dot{u}_n \Big|_0^T - \int_0^T u_n \ddot{u}_n \mathrm{d}t \right] \nonumber \\
	&=  \Re \bra{\chi} \pdv{\hat{H}}{u_n (t')} \ket{\psi} - \frac{\gamma}{2} \frac{\delta }{\delta u_n (t')} \left[ \int_0^T u_n \ddot{u}_n \mathrm{d}t \right] \nonumber \\
	&=  \Re \bra{\chi} \pdv{\hat{H}}{u_n (t')} \ket{\psi} - \frac{\gamma}{2} \frac{\delta }{\delta u_n (t')} \left[ \int_0^T \left( \pdv{u_n}{u_n (t')} \ddot{u}_n + u_n \pdv{\ddot{u}_n}{u_n (t')} \right) \mathrm{d}t \right] \nonumber \\
	&= \Re \bra{\chi} \pdv{\hat{H}}{u_n (t')} \ket{\psi} - \gamma \ddot{u}_n \; . \label{eq:deriv_u} 
\end{align} 
A solution to the equations \ref{eq:deriv_psit}, \ref{eq:deriv_psiT} and \ref{eq:deriv_u} with the initial conditions
\begin{align}
	\ket{\chi (T)} &= -i \ket{\psi_{\mathrm{target}}} \braket{\psi_{\mathrm{target}} | \psi (T)} \; , \\
	\ket{\psi (0)} &= \ket{\psi _0} \; , \\
	\boldsymbol{u}(0) = \boldsymbol{u}_1 \; &, \quad \boldsymbol{u}(T) = \boldsymbol{u}_2 \; ,
\end{align}
will satisfy equation \ref{eq:statpoint}, but only for a stationary point of $J$, which is not necessarily the minimum. For small values of $J$ however, it has been shown that the solution will be a minimum \cite{BECcontrol}. This is very difficult though, due to the complexity of the equations. An alternative is to find the minimum of the reduced cost functional $\hat{J}\left( \boldsymbol{u}(t) \right) =  J\left(\psi(\boldsymbol{u}(t)) \; , \; \boldsymbol{u}(t) \right)$, where $\psi(\boldsymbol{u}(t))$ is a unique solution of the Schrödinger equation. This can be done by iteratively updating the control using the gradient of the reduced cost functional
\begin{equation}
	\nabla \hat{J}_n = - \gamma \ddot{u}_n - \Re \bra{\chi} \pdv{\hat{H}}{u_n} \ket{\psi} \; ,
\end{equation}
which is method known as GRAPE. 

\section{Interior Point Method}
Through the GRAPE algorithm, the gradient of the cost is calculated. Thus, gradient-based optimization algorithm can be employed in the search for the control, which minimizes the cost functional. 


\section{Quantum Speed Limit}
A subtlety of the above problem is that one is only searching for the control, $\boldsymbol{u}(t)$, which steers the initial state into the target-state at time $t = T$. It is often desirable to obtain the desired state in the shortest timespan possible, however, if a solution exists at $t= T_1 > T_2$, it might not exist at $t = T_2$. The shortest duration for which a solution can be found is called the \textit{quantum speed limit} (QSL). This is due to the fact that quantum mechanics dictates that there is a limit of how many orthogonal states a system can pass through per unit time. A large energy difference to orthogonal states allows for fast oscillations within the system, however, as these differences cannot be arbitrarily large, a lower bound of how fast a system can evolve exists, which in turn leads to the QSL \cite{Caneva2009}.\\
There are several ways of approximating the quantum speed limit, however, there is no known way to reliably estimate the QSL for a general state. Thus, the best option is often to just solve the problem at increasingly shorter durations until a solution no longer can be found [JJ]. 
If the initial state,$\ket{\psi_0}$, and the target-state, $\ket{\psi_{\mathrm{target}}}$, are orthogonal, one can estimate the QSL from the orthogonalization time, which is how long it takes for a state to become orthogonal to itself.
Consider $\ket{\psi (0)} = \sum_{n}^{\infty} c_n \ket{\phi _n}$, where $\hat{H} \ket{\phi _n} = E_n \ket{\phi _n}$. Following \cite{QSLtoffoli} the norm squared of the survival probability is given as
\begin{align}
	|S(t)|^2 = |\braket{\psi (0) | \psi (t)}|^2 &= \sum_{n , m = 0}^{\infty} |c_n|^2 |c_m|^2 \cos \left( (E_n - E_m) t \right) \nonumber \\
	&\geq 1 + \frac{4 t}{\pi ^2} \dv{|S(t)|^2}{t} - \frac{4 t^2}{\pi} \Delta E^2 \; ,
\end{align}
where the trigonometric inequality $\cos x \geq 1 - \left( 4 x \sin x - 2 x^2 \right) / \pi^2$ was used, and $\Delta E$ is the energy spread of the state.
Since $|S(t)|^2 \geq 0$, then $\dv{|S(t)|^2}{t} = 0$ whenever $|S(t)| = 0$, which is the case at the orthogonalization time $t = \tau$. This leaves the inequality $0 \geq 1 - 4 \tau^2 \Delta E^2 / \pi^2$, which yields the Mandelstram Tamm bound when solved \cite{Mandelstam1991}
\begin{equation}
	\tau_{\mathrm{MT}} \geq \frac{\pi}{2 \Delta E} \; .
\end{equation}
This sets a lower bound of the orthogonalization time, however, the bound was derived using a constant Hamiltonian. In the case of optimal control the Hamiltonian is time dependent, which can be taken into account by using arguments from differential geometry \cite{Aharonov,beyondQSL}
\begin{equation}
	\tau_{\mathrm{MT}} \geq \frac{\pi}{2} \left( \int_{0}^{T} \Delta E \; \mathrm{d}t \right) ^{-1} \; , \label{eq:MTlimit}
\end{equation}
From this expression it is clear, that fast solutions require a large value of $\Delta E$, as described earlier. Since \ref{eq:MTlimit} is dependent on the control $\boldsymbol{u}(t)$, one would have to take an infimum over all controls connecting the initial and target state in order to evaluate the lowest value of the bound. This in itself is a task just as difficult as solving the control problem.


\section{Case Study of Frank article ??}
%% discuss figure of merit 
%% from Maja & Julie: infidelity much stronger figure of merit


